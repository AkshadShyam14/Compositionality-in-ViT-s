{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223aee0c-a681-4b5e-9ce2-2ff867ad723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import interpreter_login, logout\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTConfig, ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import image_utils as hf_image_utils\n",
    "from PIL import Image, ImageOps\n",
    "import scipy\n",
    "import pywt\n",
    "import pywt.data\n",
    "from torch_cka import CKA\n",
    "from torchinfo import summary\n",
    "import pickle\n",
    "from scipy.fftpack import dct, idct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import pickle\n",
    "plt.gray()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668feda7-7c7e-445b-a6ee-04a4a94250fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def class_sampler(class_id, n_samples=1):\n",
    "    class_dir = f'./dataset_classes/{class_id}/'\n",
    "    files = os.listdir(class_dir)\n",
    "    img_files = random.sample([i for i in range(50)], min(n_samples, len(files)))\n",
    "    img_paths = [f'{class_dir}{class_id}_{img_file}.jpg' for img_file in img_files]\n",
    "    return [Image.open(img_path) for img_path in img_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0c20d-1f77-497c-b184-ef9159def627",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    'base': 'google/vit-base-patch16-224',\n",
    "    'dino': 'facebook/dino-vitb16',\n",
    "    'mae': 'facebook/vit-mae-base',\n",
    "    'large': 'google/vit-large-patch16-224'\n",
    "}\n",
    "from transformers import ViTForImageClassification\n",
    "# vit_base = ViTForImageClassification.from_pretrained(model_paths['base']).to('cuda')\n",
    "# vit_dino = ViTForImageClassification.from_pretrained(model_paths['dino']).to('cuda')\n",
    "# vit_mae = ViTForImageClassification.from_pretrained(model_paths['mae']).to('cuda')\n",
    "vit_base = ViTForImageClassification.from_pretrained(model_paths['base'])\n",
    "vit_dino = ViTForImageClassification.from_pretrained(model_paths['dino'])\n",
    "vit_mae = ViTForImageClassification.from_pretrained(model_paths['mae'])\n",
    "vit_large = ViTForImageClassification.from_pretrained(model_paths['large'])\n",
    "\n",
    "base_processor = ViTImageProcessor.from_pretrained(model_paths['base'])\n",
    "dino_processor = ViTImageProcessor.from_pretrained(model_paths['dino'])\n",
    "mae_processor = ViTImageProcessor.from_pretrained(model_paths['base'])\n",
    "large_processor = ViTImageProcessor.from_pretrained(model_paths['large'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc2328-f065-4234-8e19-ae56972db60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt_decomposition(img,level):\n",
    "    \"\"\"\n",
    "    Stores the basis images corresponding to the Discrete Wavelet Transform of img\n",
    "\n",
    "    Args\n",
    "      img: a PIL Image, either RGB or Grayscale (or) a 3-channel numpy array\n",
    "      level: The level of decomposition\n",
    "\n",
    "    Returns:\n",
    "    A 1D list, containing all components\n",
    "    \"\"\"\n",
    "    # if isinstance(img, Image.Image):\n",
    "    #     np_img = np.array(img.convert('L'))\n",
    "    # else:\n",
    "    #     weights = np.array([0.299, 0.587, 0.114])\n",
    "    #     np_img = (weights[0]*img[0] + weights[1]*img[1] + weights[2]*img[2]).astype('uint8')\n",
    "    # if level == 1:\n",
    "    #     scaled_bases = pywt.dwt2(np_img, 'haar')\n",
    "    #     basis_imgs = []\n",
    "    \n",
    "    #     for i in scaled_bases:\n",
    "        \n",
    "    #         if isinstance(i, np.ndarray):\n",
    "    #             zero_coeff = np.zeros_like(i)\n",
    "    #             basis_img = pywt.idwt2(coeffs = (i, (None, None, None)), wavelet='haar')\n",
    "    #             basis_imgs.append(basis_img)\n",
    "    #         else:\n",
    "    #             for j in i:\n",
    "    #                 zero_coeff = np.zeros_like(j)\n",
    "    #                 basis_img = pywt.idwt2(coeffs = (j, (None, None, None)), wavelet='haar')\n",
    "    #                 basis_imgs.append(basis_img)\n",
    "    #     return basis_imgs\n",
    "    # else:\n",
    "    scaled_bases = pywt.wavedec2(img,'haar',level=level)\n",
    "    basis_imgs = []\n",
    "    img_shapes = []\n",
    "    for i in range(level):\n",
    "        img_shapes.append(scaled_bases[i+1][0].shape)\n",
    "    for i in range(len(scaled_bases)):\n",
    "        if isinstance(scaled_bases[i],np.ndarray):\n",
    "            coefs = [np.zeros_like(scaled_bases[0]),]  + [[np.zeros((img_shapes[k][0],img_shapes[k][1])),np.zeros((img_shapes[k][0],img_shapes[k][1])),np.zeros((img_shapes[k][0],img_shapes[k][1]))] for k in range(level)]\n",
    "            coefs[0] = scaled_bases[i]\n",
    "            basis_img = pywt.waverec2(coeffs=coefs,wavelet='haar')\n",
    "            basis_imgs.append(basis_img)\n",
    "        else:\n",
    "            for j in range(len(scaled_bases[i])):\n",
    "                # coefs = [np.zeros_like(scaled_bases[0]),] + [[np.zeros((scaled_bases[0].shape[0]*(2**k),scaled_bases[0].shape[1]*(2**k))),np.zeros((scaled_bases[0].shape[0]*(2**k),scaled_bases[0].shape[1]*(2**k))),np.zeros((scaled_bases[0].shape[0]*(2**k),scaled_bases[0].shape[1]*(2**k)))] for k in range(level)]\n",
    "                coefs = [np.zeros_like(scaled_bases[0]),] + [[np.zeros((img_shapes[k][0],img_shapes[k][1])),np.zeros((img_shapes[k][0],img_shapes[k][1])),np.zeros((img_shapes[k][0],img_shapes[k][1]))] for k in range(level)]\n",
    "                coefs[i][j] = scaled_bases[i][j]\n",
    "                basis_img = pywt.waverec2(coeffs=coefs,wavelet='haar')\n",
    "                basis_imgs.append(basis_img)\n",
    "    return basis_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8683d-b03f-49ae-a5e2-4f53489baf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_decomposition_patched(img,level, patch_dim=16):\n",
    "    \"\"\"Docstring goes here\"\"\"\n",
    "    np_img = np.array(img)\n",
    "    # weights = np.array([0.229, 0.587, 0.114])\n",
    "    # np_img = (weights[0]*img[0] + weights[1]*img[1] + weights[2]*img[2])\n",
    "    patches = np.zeros(shape=(np_img.shape[0], np_img.shape[1],np_img.shape[2], (1 + 3*level)))\n",
    "    for k in range(np_img.shape[0]):\n",
    "        for i in range(0, np_img.shape[1], patch_dim):\n",
    "            for j in range(0, np_img.shape[2], patch_dim):\n",
    "                patch_wt = wt_decomposition(np_img[k,i:i+patch_dim, j:j+patch_dim],level)\n",
    "                \n",
    "                patch = np.zeros(shape=(patch_dim, patch_dim, 1+3*level))\n",
    "                for l in range(patches.shape[3]):\n",
    "                    patch[:, :, l] = patch_wt[l]\n",
    "    \n",
    "                patches[k,i : i + patch_dim, j : j + patch_dim, :] = patch\n",
    "    return patches\n",
    "\n",
    "def wav_channel_decomposition_patched(img, level, patch_dim=16):\n",
    "    \"\"\"Docstring goes here\"\"\"\n",
    "\n",
    "    np_img = np.array(img)\n",
    "    patches = np.zeros(shape=(224, 224, 1+3*level))\n",
    "\n",
    "    for i in range(0, np_img.shape[0], patch_dim):\n",
    "        for j in range(0, np_img.shape[1], patch_dim):\n",
    "            patch_wt = wt_decomposition(np_img[i:i+patch_dim, j:j+patch_dim],level)            \n",
    "            patch = np.zeros(shape=(patch_dim, patch_dim, patch_dim**2))\n",
    "            for l in range(patch_dim):\n",
    "                for m in range(patch_dim):\n",
    "                    single_patch_dct = np.zeros_like(patch_dct)\n",
    "                    single_patch_dct[l, m] = patch_dct[l, m]\n",
    "                    patch_idct = idct2(single_patch_dct)\n",
    "                    patch[:, :, l*patch_dim + m] = patch_idct\n",
    "\n",
    "            patches[i : i + patch_dim, j : j + patch_dim, :] = patch\n",
    "    return patches, patch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83f075-7672-47e8-87dd-4e76d15df7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import pickle files\n",
    "with open('./Composition_fn_data/level_1_data_all_classes.pkl','rb') as f:\n",
    "   data =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_1_labels_all_classes.pkl','rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import pickle files\n",
    "with open('./Composition_fn_data/level_2_data_all_classes.pkl','rb') as f:\n",
    "   data_2 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_2_labels_all_classes.pkl','rb') as f:\n",
    "    labels_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle files, layer 6 data using dwt\n",
    "with open('./Composition_fn_data/level_1_data_all_classes_layer_5.pkl','rb') as f:\n",
    "   data_4 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_1_labels_all_classes_layer_5.pkl','rb') as f:\n",
    "    labels_4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc073944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impot pickle files, layer 2 using dwt\n",
    "with open('./Composition_fn_data/level_1_data_all_classes_layer_1.pkl','rb') as f:\n",
    "   data_3 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_1_labels_all_classes_layer_1.pkl','rb') as f:\n",
    "    labels_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle files, level 1 layer 12 db4\n",
    "with open('./Composition_fn_data/level_1_data_all_classes_db4.pkl','rb') as f:\n",
    "   data_5 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_1_labels_all_classes_db4.pkl','rb') as f:\n",
    "    labels_5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f77f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle files, level 2 layer 12 db4\n",
    "with open('./Composition_fn_data/level_2_data_all_classes_layer_-1_wav_db4.pkl','rb') as f:\n",
    "   data_6 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_2_labels_all_classes_layer_-1_wavdb4.pkl','rb') as f:\n",
    "    labels_6 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878f744-068b-4543-aa1e-87df4c27bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Composition_fn_data/vit_large_level_2_data_all_classes.pkl','rb') as f:\n",
    "   data_6 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/vit_large_level_2_labels_all_classes.pkl','rb') as f:\n",
    "    labels_6 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Composition_fn_data/vit_large_level_1_data_all_classes.pkl','rb') as f:\n",
    "   data_7 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/vit_large_level_1_labels_all_classes.pkl','rb') as f:\n",
    "    labels_7 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Composition_fn_data/level_2_data_all_classes_layer_1_wav_haar.pkl','rb') as f:\n",
    "   data_ =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_2_labels_all_classes_layer_1_wavhaar.pkl','rb') as f:\n",
    "    labels_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Composition_fn_data/level_2_data_all_classes_layer_5_wav_haar.pkl','rb') as f:\n",
    "   data_9 =  pickle.load(f)\n",
    "with open('./Composition_fn_data/level_2_labels_all_classes_layer_5_wavhaar.pkl','rb') as f:\n",
    "    labels_9 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7807741-d3a5-48b5-b6b3-e18bebcc6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self,data,labels,transform = True):\n",
    "        self.imgs = data\n",
    "        self.labels = labels\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        # if self.transform:\n",
    "        return self.imgs[idx],self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b10bc2-8e94-4b0c-997e-69778ef83bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Approximator(torch.nn.Module):\n",
    "    def __init__(self,level,model=vit_base):\n",
    "        super().__init__()\n",
    "        self.weight = Parameter(torch.rand(level*3 + 1))\n",
    "        # if constraint:\n",
    "        #     data = self.weight.data\n",
    "        #     data = data.clamp(min=0)\n",
    "        #     self.weight.data = data\n",
    "        # self.weight.requires_grad=True\n",
    "        self.model = model.classifier\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        if model == vit_base:\n",
    "            self.hidden_size = 768\n",
    "        elif model == vit_large:\n",
    "            self.hidden_size = 1024\n",
    "    def forward(self,input: torch.Tensor)-> torch.Tensor:\n",
    "        print(input.shape)\n",
    "        if input.shape[-1] == self.hidden_size:\n",
    "            input = input.permute(0,-1,1)\n",
    "        print(torch.matmul(input,self.weight.data))\n",
    "        out = F.linear(input,self.weight)\n",
    "        return self.model(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705aa3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Approximator(torch.nn.Module):\n",
    "    def __init__(self,level,model=vit_base):\n",
    "        super().__init__()\n",
    "        self.weight = Parameter(torch.rand(level*3 + 1))\n",
    "        # if constraint:\n",
    "        #     data = self.weight.data\n",
    "        #     data = data.clamp(min=0)\n",
    "        #     self.weight.data = data\n",
    "        # self.weight.requires_grad=True\n",
    "        self.model = model.classifier\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        if model == vit_base:\n",
    "            self.hidden_size = 768\n",
    "        elif model == vit_large:\n",
    "            self.hidden_size = 1024\n",
    "    def forward(self,input: torch.Tensor)-> torch.Tensor:\n",
    "        if input.shape[-1] == self.hidden_size:\n",
    "            input = input.permute(0,-1,1)\n",
    "        out = F.linear(input,self.weight)\n",
    "        #out = input * self.weight.unsqueeze(0).unsqueeze(-1) \n",
    "        return self.model(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9d3d6-8a4f-452b-89e7-1b88be31abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index,model,train_loader,loss_fn,optimizer,softmax,constraint=False):\n",
    "    running_loss = 0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to('cpu'))\n",
    "        loss = loss_fn(softmax(outputs),softmax(labels[:,0,:].to('cpu')))\n",
    "        loss.backward()\n",
    "        for i,param in enumerate(model.parameters()):\n",
    "            if i != 0:\n",
    "                param.requires_grad = False\n",
    "        optimizer.step()\n",
    "        if constraint == 'Non-negative':\n",
    "            with torch.no_grad():\n",
    "                model.weight.copy_ (model.weight.data.clamp(min=0))\n",
    "        # elif constraint == 'convex':\n",
    "        #     soft_fn = torch.nn.Softmax()\n",
    "            # with torch.no_grad():\n",
    "            #     model.weight.copy_ (soft_fn(model.weight.data))\n",
    "        running_loss += loss.item()  \n",
    "    return running_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4108d00-3c53-43b1-b134-84b8f525c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epochs,train_loader,val_loader,loss_fn,optimizer,softmax,constraint=False):\n",
    "    epoch_number = 0\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    \n",
    "    best_vloss = 1_000_000.\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number,model,train_loader,loss_fn,optimizer,softmax,constraint)\n",
    "        # print(avg_loss)\n",
    "        print(model.weight)\n",
    "        running_vloss = 0.0\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "            # Make predictions for this batch\n",
    "                com_cls = model(vinputs.to('cpu'))\n",
    "    \n",
    "                vloss = loss_fn(softmax(com_cls),softmax(vlabels[:,0,:].to('cpu')))\n",
    "                running_vloss += vloss\n",
    "                compose_cls = torch.argmax(com_cls).item()\n",
    "                org_cls = torch.argmax(vlabels).item()\n",
    "                if compose_cls == org_cls:\n",
    "                    acc+=1\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {} acc {}'.format(avg_loss, avg_vloss,acc))\n",
    "    \n",
    "        if avg_vloss < best_vloss:\n",
    "    \n",
    "            best_vloss = avg_vloss\n",
    "        epoch_number += 1\n",
    "    if constraint == 'convex':\n",
    "        soft_fn = torch.nn.Softmax()\n",
    "        with torch.no_grad():\n",
    "            model.weight.copy_ (soft_fn(model.weight.data))\n",
    "    return model,acc,avg_loss,avg_vloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c74dfa-3075-4d1d-9f08-c81876e5c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            inputs,labels = vdata\n",
    "            com_cls = model(inputs.to('cpu'))\n",
    "            compose_cls = torch.argmax(com_cls).item()\n",
    "            org_cls = torch.argmax(labels).item()\n",
    "            if compose_cls == org_cls:\n",
    "                acc+=1\n",
    "        return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47399468-3175-40f7-817c-5a575c7804aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data,labels,vit = vit_base,test_size = 0.3,level = 1,batch_size = 100,loss = 'CE',optim = 'SGD',epochs = 200,lr=0.001,constraint=False):\n",
    "    #Split the dataset into train,test,val\n",
    "    X_train,X_test,y_train,y_test = train_test_split(data,labels,test_size = test_size,random_state=42)\n",
    "    X_val,X_test,y_val,y_test = train_test_split(X_test,y_test,test_size = 0.5,random_state=42)\n",
    "    #The labels are all on gpu so we transfer them to cpu\n",
    "    for i in range(len(y_train)):\n",
    "        y_train[i] = y_train[i].cpu()\n",
    "\n",
    "    for i in range(len(y_val)):\n",
    "        y_val[i] = y_val[i].cpu()\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        y_test[i] = y_test[i].cpu()\n",
    "    \n",
    "    #Wrap the datasets in pytorch Dataset Class\n",
    "    train_data = Data(X_train,y_train)\n",
    "    val_data = Data(X_val,y_val)\n",
    "    test_data = Data(X_test,y_test)\n",
    "\n",
    "    #Dataloaders for train,test,val\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_data,batch_size=1,shuffle=True)\n",
    "\n",
    "    #Instance of our model\n",
    "    model = Approximator(level=level,model=vit)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr = lr)\n",
    "    elif optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    #Softmax for the output of model and labels\n",
    "    soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    model,val_acc,avg_loss,avg_vloss = train(model,epochs,train_loader,val_loader,loss_fn,optimizer,soft,constraint)\n",
    "\n",
    "    test_acc = test(model,test_loader)\n",
    "    return model,val_acc/len(val_loader),test_acc/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cd622-5446-4b53-8ea7-079039b7421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To use constraints \n",
    "## 1) Non negative - pass constraint = 'Non-negative'\n",
    "## 2) Convex - pass constraint = 'convex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d53002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,val_acc,test_acc = train_model(data_6, labels_6, level = 2, epochs = 100, batch_size= 100, optim='SGD',lr=0.001,constraint=False, vit = vit_large) \n",
    "print(\"Test_acc\", test_acc,\"Val_acc\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Composition_fn_data/level_2_layer_2_SGD.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f994cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Approximator(level=2, model = vit_large)\n",
    "model.load_state_dict(torch.load('./Composition_fn_data/level_2_layer_2_SGD.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,val_acc,test_acc = train_model(data_, labels_, level = 1, epochs = 100, batch_size= 100, optim='SGD',lr=0.001,constraint='Non-negative') \n",
    "print(\"Test_acc\", test_acc,\"Val_acc\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Composition_fn_data/level_2_layer_2_SGD_NonNeg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22177f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nocon = Approximator(level=2)\n",
    "model_nocon.load_state_dict(torch.load('./Composition_fn_data/level_2_db4_Adam.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c617419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nocon.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,val_acc,test_acc = train_model(data_6, labels_6, level = 2, epochs = 100, batch_size= 100, optim='Adam',lr=0.001,constraint='Non-negative') \n",
    "print(\"Test_acc\", test_acc,\"Val_acc\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Composition_fn_data/level_2_db4_Adam_NonNeg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NonNeg = Approximator(level=2)\n",
    "model_NonNeg.load_state_dict(torch.load('./Composition_fn_data/level_2_db4_Adam_NonNeg.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NonNeg.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13efddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,val_acc,test_acc = train_model(data_6, labels_6, level = 2, epochs = 100, batch_size= 100, optim='Adam',lr=0.001,constraint='convex') \n",
    "print(\"Test_acc\", test_acc,\"Val_acc\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Composition_fn_data/level_2_db4_Adam_con.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_con = Approximator(level=2)\n",
    "model_con.load_state_dict(torch.load('./Composition_fn_data/level_2_db4_Adam_con.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_con.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
